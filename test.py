import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from opd.src.utils.utils import compute_logprobs_from_model, generate_completions

def load_model_and_tokenizer(model_path, device):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # Decoder-only models need left padding for correct generation
    tokenizer.padding_side = "left"

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=device
    )
    return model, tokenizer

student_model_path = "/mnt/nvme0/tsz/modelscope_cache/models/Qwen/Qwen3-0.6B"
teacher_model_path = "/mnt/nvme0/tsz/modelscope_cache/models/Qwen/Qwen3-8B"
device = "cuda"
print("Loading student tokenizer and model...")
student_model, student_tokenizer = load_model_and_tokenizer(student_model_path, device)
student_model.eval()
student_model.requires_grad_(False)
print("Loading teacher tokenizer and model...")
teacher_model, teacher_tokenizer = load_model_and_tokenizer(teacher_model_path, device)
teacher_model.eval()
teacher_model.requires_grad_(False)

# prompt = "prompt"
# tokenized = student_tokenizer(prompt, max_length=512, padding="max_length", truncation=True, return_tensors="pt")
# input_ids = tokenized["input_ids"]
# attention_mask = tokenized["attention_mask"]
# generated_ids = [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,14582, 25, 7801, 374, 264, 1156, 4666, 5458, 518, 264, 3822, 304, 10557, 13, 1260, 702, 264, 8039, 315, 400, 16, 15, 15, 15, 817, 33153, 13,1260, 37102, 220, 18, 15, 4, 315, 806, 3220, 389, 3607, 11, 220, 16, 20, 4, 389, 27278, 11, 220, 17, 20, 4, 389, 16517, 11, 323, 279, 2732, 389, 74433, 7236, 13, 2585, 1753, 3220,1558, 566, 8329, 389, 74433, 7236, 5267, 16141, 25,6771, 594, 11625, 419, 3019, 553, 3019, 624, 3586, 47, 81401, 43, 3920, 39, 2884, 35, 2884, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35,6979, 35, 6979, 35, 6979, 35, 6979, 35, 6979, 35]
generated_ids = [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,151643, 151643, 151643, 151643, 151643, 151643, 151643, 14582, 25, 7801, 374, 264, 1156, 4666, 5458, 518, 264, 3822, 304, 10557, 13, 1260, 702, 264, 8039, 315, 400, 16, 15, 15, 15, 817, 33153, 13, 1260, 37102, 220, 18, 15, 4, 315, 806, 3220, 389, 3607, 11, 220, 16, 20, 4, 389, 27278, 11, 220, 17, 20, 4, 389, 16517, 11, 323, 279, 2732, 389, 74433, 7236, 13, 2585, 1753, 3220, 1558, 566, 8329, 389, 74433, 7236, 5267, 16141, 25, 6771, 594, 11625, 419, 3019, 553, 3019, 624, 5338, 11, 1077, 594, 11047, 279, 3311, 315, 3220, 566, 37102, 389, 3607, 624, 1249, 1477, 279, 3311, 7391, 389, 3607, 25, 220, 18, 15, 4, 315, 400, 16, 15, 15, 15, 284, 400, 18, 15, 15, 198, 12209, 11, 1077, 594, 11047, 279, 3311, 315, 3220, 566, 37102, 389, 27278, 25, 220, 16, 20, 4, 315, 400, 16, 15, 15, 15, 284, 400, 16, 20, 15, 198, 5847, 11, 1077, 594, 11047, 279, 3311, 315, 3220, 566, 37102, 389, 16517, 25, 220, 17, 20, 4, 315, 400, 16, 15, 15, 15, 284, 400, 17, 20, 15, 198, 7039, 11, 1077, 594, 11047, 279, 3311, 315, 3220, 566, 37102, 389, 74433, 7236, 25, 400, 16, 15, 15, 15, 481, 400, 18, 15, 15, 481, 400, 16, 20, 15]
generated_ids = torch.tensor([generated_ids]).to(device)
print("decoded: ", teacher_tokenizer.decode(generated_ids))
seq_ids, seq_mask = generated_ids, (generated_ids != teacher_tokenizer.pad_token_id).long()
with torch.no_grad():
    student_logprobs = compute_logprobs_from_model(student_model, seq_ids, seq_mask)
    print(f"student logprobs: {student_logprobs.shape}, {student_logprobs}")
    print(f"student logprobs mean: {student_logprobs.mean(dim=1)}")
    teacher_logprobs = compute_logprobs_from_model(teacher_model, seq_ids, seq_mask)
    print(f"teacher logprobs: {teacher_logprobs.shape}, {teacher_logprobs}")
    print(f"teacher logprobs mean: {student_logprobs.mean(dim=1)}")
    # outputs = generate_completions(teacher_model, teacher_tokenizer, seq_ids[:, :-6], seq_mask, max_new_tokens=6, num_samples=6)
    # print(outputs['completions'])
    breakpoint()
# -11.6875